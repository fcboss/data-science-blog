{
  
    
        "post0": {
            "title": "Syncing Audiobooks and  Ebooks with Speech Recognition",
            "content": "I&#39;m a huge fan of audiobooks because of how faster it is to listen to an audiobook than actually reading it. However, I&#39;ve always felt that I absorbed the material much better when I put the time to sit down and read the book. Some recent studies also suggest the reality of this phenomenon (see here). Therefore, the concept of reading and listening at the same time has been appealing. . In the past few years, the only way to do this would be to get the audiobook and the ebook separately and to follow along with the book while the audio played in the background. But a much better way to smooth this experience would be to have the ebook autoscroll to the speed of the spoken word. Audible introduced their product called Immersive Reading in 2012, but it only became popular in the last few years and its price is not great. . . . To the best of my knowledge, there are currently no free softwares that offer this service and so I started wondering how hard could it be. . Preliminary Brainstorm . The problem consists in finding, for a given portion of the audiobook, the location of the corresponding text in the book. . I was very optimistic that I could solve a problem like this a priori, because we do not need our model to work very well at the word level (as it is usual in speech processing), but we only need to get roughly right which sentence the audio is pointg to and then scroll to that paragraph. It was also clear that the span of sentences to be looked for at each time is not the whole book since, in the first seconds of the audiobook, the corresponding text will also be in the first sentences of the file. . So, our model should receive a chunk of the audiobook and a set of chuncks of text as input and it should predict which chunk of text best fits the audio. . Some of the first few ideas for solutions and possible obstacles from the initial brainstorm consisted in: . Splitting the audio in equal time spans vs detecting the limits of each sentence/ word. | Converting the audio chuncks to spectograms and use that to train a neural network such as: A CNN | An RNN | A Siamese model | . | Using a pretrained model to predict the text for each audio and use that as the input of a simpler model. | . The last one was obviously the first one to try, even though the junior data scientist instinct kicked in by making me dream with different NN architectures for solving a possibly simple model. .",
            "url": "https://fcboss.github.io/data-science-blog/2020/12/08/Scroller.html",
            "relUrl": "/2020/12/08/Scroller.html",
            "date": " • Dec 8, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "1. Import libraries",
            "content": "import time import numpy as np # data manipulation import pandas as pd from sklearn.model_selection import train_test_split from sklearn.model_selection import cross_val_score # plot visualization import seaborn as sns import matplotlib.pyplot as plt from IPython.display import display # classifiers from sklearn.ensemble import RandomForestClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn import svm . 2. Breast Cancer Wisconsin (diagnostic) dataset . Attribute Information: . 1) ID number . 2) Diagnosis (M = malignant, B = benign) . 3-32) Ten real-valued features are computed for each cell nucleus: . a) Radius (mean of distances from center to points on the perimeter) . b) Texture (standard deviation of gray-scale values) . c) Perimeter . d) Area . e) Smoothness (local variation in radius lengths) . f) Compactness (perimeter^2 / area - 1.0) . g) Concavity (severity of concave portions of the contour) . h) Concave points (number of concave portions of the contour) . i) Symmetry . j) Fractal dimension (&quot;coastline approximation&quot;) . The mean, standard error and &quot;worst&quot; or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius. . 2.1. Read csv file . data = pd.read_csv(&quot;breast-cancer.csv&quot;) display(data.columns) display(data.info()) data.head() . Index([&#39;id&#39;, &#39;diagnosis&#39;, &#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;, &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;, &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;, &#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;, &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;, &#39;fractal_dimension_se&#39;, &#39;radius_worst&#39;, &#39;texture_worst&#39;, &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;, &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;, &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;, &#39;Unnamed: 32&#39;], dtype=&#39;object&#39;) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 569 entries, 0 to 568 Data columns (total 33 columns): id 569 non-null int64 diagnosis 569 non-null object radius_mean 569 non-null float64 texture_mean 569 non-null float64 perimeter_mean 569 non-null float64 area_mean 569 non-null float64 smoothness_mean 569 non-null float64 compactness_mean 569 non-null float64 concavity_mean 569 non-null float64 concave points_mean 569 non-null float64 symmetry_mean 569 non-null float64 fractal_dimension_mean 569 non-null float64 radius_se 569 non-null float64 texture_se 569 non-null float64 perimeter_se 569 non-null float64 area_se 569 non-null float64 smoothness_se 569 non-null float64 compactness_se 569 non-null float64 concavity_se 569 non-null float64 concave points_se 569 non-null float64 symmetry_se 569 non-null float64 fractal_dimension_se 569 non-null float64 radius_worst 569 non-null float64 texture_worst 569 non-null float64 perimeter_worst 569 non-null float64 area_worst 569 non-null float64 smoothness_worst 569 non-null float64 compactness_worst 569 non-null float64 concavity_worst 569 non-null float64 concave points_worst 569 non-null float64 symmetry_worst 569 non-null float64 fractal_dimension_worst 569 non-null float64 Unnamed: 32 0 non-null float64 dtypes: float64(31), int64(1), object(1) memory usage: 146.8+ KB . None . id diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean ... texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst Unnamed: 32 . 0 842302 | M | 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | ... | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | NaN | . 1 842517 | M | 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | ... | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | NaN | . 2 84300903 | M | 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | ... | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | NaN | . 3 84348301 | M | 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | ... | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | NaN | . 4 84358402 | M | 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | ... | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | NaN | . 5 rows × 33 columns . Observations: . By observing the information about the dataset we can see that the diagnosis column corresponds to the true label of the samples so we will label malign diagnosis as 1 and benign diagnosis as 0. Also, we can drop the columns id and Unnamed: 32 because the first one has information about the IDs of the samples which do not give us useful information and the second is a column with NaN values. . data[&#39;diagnosis&#39;]=data[&#39;diagnosis&#39;].map({&#39;M&#39;:1,&#39;B&#39;:0}) data = data.drop([&#39;id&#39;,&#39;Unnamed: 32&#39;],axis = 1 ) data.head() . diagnosis radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean ... radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst . 0 1 | 17.99 | 10.38 | 122.80 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | ... | 25.38 | 17.33 | 184.60 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | . 1 1 | 20.57 | 17.77 | 132.90 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | ... | 24.99 | 23.41 | 158.80 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | . 2 1 | 19.69 | 21.25 | 130.00 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | ... | 23.57 | 25.53 | 152.50 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | . 3 1 | 11.42 | 20.38 | 77.58 | 386.1 | 0.14250 | 0.28390 | 0.2414 | 0.10520 | 0.2597 | ... | 14.91 | 26.50 | 98.87 | 567.7 | 0.2098 | 0.8663 | 0.6869 | 0.2575 | 0.6638 | 0.17300 | . 4 1 | 20.29 | 14.34 | 135.10 | 1297.0 | 0.10030 | 0.13280 | 0.1980 | 0.10430 | 0.1809 | ... | 22.54 | 16.67 | 152.20 | 1575.0 | 0.1374 | 0.2050 | 0.4000 | 0.1625 | 0.2364 | 0.07678 | . 5 rows × 31 columns . 2.2. Split dataset into train a test set . train, test = train_test_split(data, test_size=0.3, random_state=10) train_y = train.diagnosis test_y = test.diagnosis print(&quot;Train size: {} samples.&quot;.format(len(train))) print(&quot;Test size: {} samples.&quot;.format(len(test))) train = train.drop([&#39;diagnosis&#39;],axis = 1 ) test = test.drop([&#39;diagnosis&#39;],axis = 1 ) . Train size: 398 samples. Test size: 171 samples. . Observations: . We splitted the dataset in 70% for train and 30% for test. . We are going to store the train and test true label into train_y and test_y variables. After doing that, we can drop diagnosis column from the train and test dataset. . The train_test_split function from sklearn shuffles the data before splitting. . benign, malign = train_y.value_counts() print(&quot;Benign samples: {}.&quot;.format(benign)) print(&quot;Malign samples: {}.&quot;.format(malign)) sns.countplot(train_y,label=&quot;Count&quot;) . Benign samples: 245. Malign samples: 153. . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ccd0c56c88&gt; . 3. Feature selection . Since the 30 features computed for each cell nucleous are divided into three sets of ten real-valued features, we are going to analyze each set first. . mean_features = [&#39;radius_mean&#39;, &#39;texture_mean&#39;, &#39;perimeter_mean&#39;, &#39;area_mean&#39;, &#39;smoothness_mean&#39;, &#39;compactness_mean&#39;, &#39;concavity_mean&#39;, &#39;concave points_mean&#39;, &#39;symmetry_mean&#39;, &#39;fractal_dimension_mean&#39;] se_features = [&#39;radius_se&#39;, &#39;texture_se&#39;, &#39;perimeter_se&#39;, &#39;area_se&#39;, &#39;smoothness_se&#39;, &#39;compactness_se&#39;, &#39;concavity_se&#39;, &#39;concave points_se&#39;, &#39;symmetry_se&#39;, &#39;fractal_dimension_se&#39;] worst_features = [&#39;radius_worst&#39;, &#39;texture_worst&#39;, &#39;perimeter_worst&#39;, &#39;area_worst&#39;, &#39;smoothness_worst&#39;, &#39;compactness_worst&#39;, &#39;concavity_worst&#39;, &#39;concave points_worst&#39;, &#39;symmetry_worst&#39;, &#39;fractal_dimension_worst&#39;] . 3.1. Mean features . train[mean_features].describe() . radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean concave points_mean symmetry_mean fractal_dimension_mean . count 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | . mean 14.258538 | 19.233467 | 92.939271 | 667.674874 | 0.096756 | 0.106870 | 0.091604 | 0.050603 | 0.181022 | 0.062844 | . std 3.574314 | 4.284989 | 24.670157 | 362.391686 | 0.013609 | 0.051948 | 0.078648 | 0.039083 | 0.026387 | 0.007010 | . min 6.981000 | 10.380000 | 43.790000 | 143.500000 | 0.052630 | 0.019380 | 0.000000 | 0.000000 | 0.106000 | 0.049960 | . 25% 11.750000 | 16.237500 | 76.110000 | 426.175000 | 0.087452 | 0.068125 | 0.032808 | 0.021895 | 0.163200 | 0.058112 | . 50% 13.475000 | 18.760000 | 87.380000 | 560.100000 | 0.096865 | 0.097535 | 0.067260 | 0.036565 | 0.179250 | 0.061740 | . 75% 15.832500 | 21.587500 | 104.250000 | 790.575000 | 0.105375 | 0.130900 | 0.135475 | 0.075667 | 0.196500 | 0.066143 | . max 28.110000 | 39.280000 | 188.500000 | 2501.000000 | 0.144700 | 0.311400 | 0.426800 | 0.201200 | 0.304000 | 0.097440 | . As we can observe, the range of values of each feature varies (even for the se and worst features as you can observe in sections 3.2. and 3.3.). For example, the mean value for area_mean is, approximately, 668.86 whereas the mean value of concavity mean is, approximately, 0.093. Thus, we need to do standarization before analyzing the data and doing classification. . train_std = (train - train.mean()) / train.std() test_std = (test - test.mean()) / test.std() . f,ax = plt.subplots(figsize=(10, 10)) sns.heatmap(train[mean_features].corr(), annot=True, linewidths=.5, fmt= &#39;.1f&#39;,ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ccd0d81e48&gt; . Observations: . The radius_mean, perimeter_mean and area_mean are highly correlated as expected from their relation. We will later choose one of them. . The compactness_mean, concavity_mean and concave points_mean are highly correlated as well. Thus, we will choose one of them too. . sns.set(style=&quot;whitegrid&quot;, palette=&quot;muted&quot;) data = pd.concat([train_y.map({1:&#39;M&#39;,0:&#39;B&#39;}),train_std[mean_features]],axis=1) data = pd.melt(data,id_vars=&quot;diagnosis&quot;, var_name=&quot;Mean features&quot;, value_name=&#39;Value&#39;) plt.figure(figsize=(12,12)) sns.swarmplot(x=&quot;Mean features&quot;, y=&quot;Value&quot;, hue=&quot;diagnosis&quot;, data=data) plt.xticks(rotation=90) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), &lt;a list of 10 Text xticklabel objects&gt;) . Observations: . The features radius mean, perimeter_mean and area_mean look are very clear in terms of classification. There is not of them that seperates clearly better than the others, if it was we would choose that feature. Thus, we will choose one of the 3, let it be area_mean. . Regarding the correlation of compactness_mean, concavity_mean and concave points_mean, we see that compactness_mean doest not seperate the classes as clear as the other two. The concave points_mean seems to be the one that seperates data better, so we will choose it. . The features symmetry_mean and fractal_dimension_mean do not seperate classes clearly. However, we will keep them for now. . So, the following 4 features are going to be dropped: radius mean, perimeter_mean, compactness_mean and concavity_mean. . train = train.drop([&#39;radius_mean&#39;, &#39;perimeter_mean&#39;,&#39;compactness_mean&#39;, &#39;concavity_mean&#39;], axis=1) test = test.drop([&#39;radius_mean&#39;, &#39;perimeter_mean&#39;,&#39;compactness_mean&#39;, &#39;concavity_mean&#39;], axis=1) . 3.2. SE features . train[se_features].describe() . radius_se texture_se perimeter_se area_se smoothness_se compactness_se concavity_se concave points_se symmetry_se fractal_dimension_se . count 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | . mean 0.419068 | 1.221311 | 2.989503 | 42.198307 | 0.007146 | 0.026148 | 0.032015 | 0.012062 | 0.020351 | 0.003864 | . std 0.297235 | 0.548427 | 2.173327 | 49.688806 | 0.003181 | 0.017663 | 0.024343 | 0.006151 | 0.007644 | 0.002584 | . min 0.111500 | 0.360200 | 0.757000 | 7.228000 | 0.002667 | 0.002252 | 0.000000 | 0.000000 | 0.007882 | 0.000895 | . 25% 0.237675 | 0.837200 | 1.680250 | 18.217500 | 0.005215 | 0.013822 | 0.015315 | 0.008009 | 0.015360 | 0.002296 | . 50% 0.333350 | 1.127000 | 2.372500 | 25.080000 | 0.006431 | 0.020790 | 0.026335 | 0.011365 | 0.018820 | 0.003134 | . 75% 0.507375 | 1.473750 | 3.472750 | 44.867500 | 0.008301 | 0.033473 | 0.043103 | 0.015163 | 0.023283 | 0.004588 | . max 2.873000 | 3.896000 | 21.980000 | 542.200000 | 0.031130 | 0.106400 | 0.153500 | 0.040900 | 0.061460 | 0.022860 | . f,ax = plt.subplots(figsize=(10, 10)) sns.heatmap(train[se_features].corr(), annot=True, linewidths=.5, fmt= &#39;.1f&#39;,ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ccd36b1cf8&gt; . Observations: . The radius_se, perimeter_se and area_se are highly correlated as expected from their relation. We will later choose one of them. . The compactness_se, concavity_se and concave points_se are highly correlated as well. Also, note that fractal_dimension_se is highly correlated with these 3, especially with compactness_se and concavity_se. . sns.set(style=&quot;whitegrid&quot;, palette=&quot;muted&quot;) data = pd.concat([train_y.map({1:&#39;M&#39;,0:&#39;B&#39;}),train_std[se_features]],axis=1) data = pd.melt(data,id_vars=&quot;diagnosis&quot;, var_name=&quot;SE features&quot;, value_name=&#39;Value&#39;) plt.figure(figsize=(12,12)) sns.swarmplot(x=&quot;SE features&quot;, y=&quot;Value&quot;, hue=&quot;diagnosis&quot;, data=data) plt.xticks(rotation=90) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), &lt;a list of 10 Text xticklabel objects&gt;) . Observations: . The features radius se, perimeter_se and area_se are very clear in terms of classification. The area_se seems to be the one that seperates data better, so we will choose it. . Regarding the correlation of compactness_se, concavity_se, concave points_se and fractal_dimension_se, we see that fractal_dimension_se does not seperate the classes clearly. Since compactness_se and concavity_se had the highest correlation values regarding these 4 features we will choose one of these two. In addition, concavity_se seperates the classes clearly so we will pick this one. . The features texture_se, smothness_se and symmetry_se do not seperate classes clearly. However, we will keep them for now. . So, the following 5 features are going to be dropped: radius_se, perimeter_se, compactness_se, concave points_se and fractal_dimension_se. . train = train.drop([&#39;radius_se&#39;, &#39;perimeter_se&#39;,&#39;compactness_se&#39;, &#39;concave points_se&#39;, &#39;fractal_dimension_se&#39;], axis=1) test = test.drop([&#39;radius_se&#39;, &#39;perimeter_se&#39;,&#39;compactness_se&#39;, &#39;concave points_se&#39;, &#39;fractal_dimension_se&#39;], axis=1) . 3.3. Worst features . train[worst_features].describe() . radius_worst texture_worst perimeter_worst area_worst smoothness_worst compactness_worst concavity_worst concave points_worst symmetry_worst fractal_dimension_worst . count 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | 398.000000 | . mean 16.442696 | 25.602965 | 108.688568 | 898.009296 | 0.132906 | 0.260155 | 0.278177 | 0.117286 | 0.289159 | 0.084246 | . std 4.854101 | 6.011069 | 33.854294 | 577.877208 | 0.021689 | 0.153249 | 0.203433 | 0.064576 | 0.057824 | 0.018019 | . min 7.930000 | 12.490000 | 50.410000 | 185.200000 | 0.081250 | 0.034320 | 0.000000 | 0.000000 | 0.156500 | 0.055040 | . 25% 13.102500 | 21.392500 | 84.757500 | 521.550000 | 0.119225 | 0.154300 | 0.127550 | 0.066865 | 0.251100 | 0.072462 | . 50% 15.110000 | 25.265000 | 99.260000 | 706.000000 | 0.132350 | 0.223550 | 0.239650 | 0.105600 | 0.281650 | 0.080595 | . 75% 18.707500 | 29.485000 | 126.750000 | 1065.000000 | 0.146075 | 0.339200 | 0.386400 | 0.162200 | 0.316525 | 0.092140 | . max 36.040000 | 49.540000 | 251.200000 | 4254.000000 | 0.209800 | 1.058000 | 1.170000 | 0.286700 | 0.663800 | 0.207500 | . f,ax = plt.subplots(figsize=(10, 10)) sns.heatmap(train[worst_features].corr(), annot=True, linewidths=.5, fmt= &#39;.1f&#39;,ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ccd3804eb8&gt; . Observations: . The radius_worst, perimeter_worst and area_worst are highly correlated as expected from their relation. We will later choose one of them. . The compactness_worst, concavity_worst and concave points_worst are highly correlated as well. Also, note that fractal_dimension_worst is highly correlated with these 3, especially with compactness_worst and concavity_worst. . sns.set(style=&quot;whitegrid&quot;, palette=&quot;muted&quot;) data = pd.concat([train_y.map({1:&#39;M&#39;,0:&#39;B&#39;}),train_std[worst_features]],axis=1) data = pd.melt(data,id_vars=&quot;diagnosis&quot;, var_name=&quot;Worst features&quot;, value_name=&#39;Value&#39;) plt.figure(figsize=(12,12)) sns.swarmplot(x=&quot;Worst features&quot;, y=&quot;Value&quot;, hue=&quot;diagnosis&quot;, data=data) plt.xticks(rotation=90) . (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), &lt;a list of 10 Text xticklabel objects&gt;) . Observations: . The features radius_worst, perimeter_worst and area_worst are very clear in terms of classification. The area_worst seems to be the one that seperates data better, so we will choose it. . Regarding the correlation of compactness_worst, concavity_worst, concave points_worst and fractal_dimension_worst, we see that fractal_dimension_worst does not seperate the classes clearly. Since compactness_worst and concavity_worst had the highest correlation values regarding these 4 features we will choose one of these two. In addition, concavity_worst seperates the classes clearly so we will pick this one. . So, the following 5 features are going to be dropped: radius_worst, perimeter_worst, compactness_worst, concave points_worst and fractal_dimension_worst. . train = train.drop([&#39;radius_worst&#39;, &#39;perimeter_worst&#39;,&#39;compactness_worst&#39;, &#39;concave points_worst&#39;, &#39;fractal_dimension_worst&#39;], axis=1) test = test.drop([&#39;radius_worst&#39;, &#39;perimeter_worst&#39;,&#39;compactness_worst&#39;, &#39;concave points_worst&#39;, &#39;fractal_dimension_worst&#39;], axis=1) . 3.4. Correlation between all remaining features . f,ax = plt.subplots(figsize=(14, 14)) sns.heatmap(train.corr(), annot=True, linewidths=.5, fmt= &#39;.1f&#39;,ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1ccd44b98d0&gt; . Observations: . We still have some highly correlated features. The feature texture_mean and texture_worst are correlated as well as area_mean and area_worst, we will choose texture_mean and area_mean. . train = train.drop([&#39;texture_worst&#39;,&#39;area_worst&#39;], axis=1) test = test.drop([&#39;texture_worst&#39;,&#39;area_worst&#39;], axis=1) . 4. Classification . 4.1. Support Vector Machine . gamma = [0.1, 1, 10, 100] # gamma coefficient for RBF and polynomial (higher values tend to overfit data) C = [0.1, 1, 10, 100] # different penalties of the error term kernel = [&#39;rbf&#39;,&#39;poly&#39;] # different kernels degree = [1, 2, 3, 4] # degree of polynomial kernel best_score = 0 best_values = {} start = time.time() for i in range(50): hyp1 = gamma[i] hyp2 = C[j] hyp3 = kernel[l] hyp4 = degree[k] clf = svm.SVC(kernel=hyp3, gamma=hyp1, C=hyp2, degree=hyp4) score = cross_val_score(clf, train, train_y, cv=5).mean() if score &gt; best_score: best_score = score best_values[&#39;gamma&#39;] = hyp1 best_values[&#39;C&#39;] = hyp2 best_values[&#39;kernel&#39;] = hyp3 best_values[&#39;degree&#39;] = hyp4 partial = time.time() print(&quot;New best set of values:&quot;) print(best_values) print(&quot;Mean accuracy = &quot;+str(round(score,4))+&quot; (with 5-fold cross validation).&quot;) print(&quot; found after &quot; + str(round(partial - start,2)) + &quot; seconds n&quot;) end = time.time() clf_svm = svm.SVC(kernel=&#39;rbf&#39;, gamma=best_values[&#39;gamma&#39;], C=best_values[&#39;C&#39;]) print(&quot;The best values are:&quot;) print(best_values) print(&quot;Best accuracy = &quot; + str(round(best_score,4))+&quot; (with 5-fold cross validation).&quot;) print(&quot; found in &quot; + str(round(end - start,2)) + &quot; seconds&quot;) . 0.1 0.1 rbf 1 New best set of values: {&#39;gamma&#39;: 0.1, &#39;C&#39;: 0.1, &#39;kernel&#39;: &#39;rbf&#39;, &#39;degree&#39;: 1} Mean accuracy = 0.616 found after 0.06 seconds 0.1 0.1 rbf 2 0.1 0.1 poly 1 0.1 0.1 poly 2 0.1 1 rbf 1 New best set of values: {&#39;gamma&#39;: 0.1, &#39;C&#39;: 1, &#39;kernel&#39;: &#39;rbf&#39;, &#39;degree&#39;: 1} Mean accuracy = 0.613 found after 146.13 seconds 0.1 1 rbf 2 0.1 1 poly 1 0.1 1 poly 2 0.1 10 rbf 1 0.1 10 rbf 2 0.1 10 poly 1 0.1 10 poly 2 . Observations: . We are applying 5-fold cross-validation to the train set before doing the prediction in the test set. Also, we are applying Grid Search to the SVM classifier because there are only 16 different combinations of the defined hyperparameters. (TODO: explicar cada um deles). . The best mean accuracy estimated with cross-validation is: . The best set of hyperparameters is: . 4.2. Random forest . min_samples_leaf_values = np.arange(1, 50, 1) min_samples_split_values = np.arange(2, 50, 1) max_depth_values = np.arange(1, 100, 1) n_estimators_values = np.arange(80,200,1) best_score = 0 best_values = {} start = time.time() for i in range(1,100): hyp1 = np.random.choice(a=min_samples_leaf_values) hyp2 = np.random.choice(a=min_samples_split_values) hyp3 = np.random.choice(a=max_depth_values) hyp4 = np.random.choice(a=n_estimators_values) clf = RandomForestClassifier(random_state=10, min_samples_leaf=hyp1, min_samples_split=hyp2, max_depth=hyp3, n_estimators=hyp4).fit(train, train_y) score = cross_val_score(clf, train, train_y, cv=5).mean() if score &gt; best_score: best_score = score best_values[&#39;min_sample_leaf&#39;] = hyp1 best_values[&#39;min_sample_split&#39;] = hyp2 best_values[&#39;max_depth&#39;] = hyp3 best_values[&#39;n_estimators&#39;] = hyp4 partial = time.time() print(&quot;New best set of values (iteration {})&quot;.format(i)) print(best_values) print(&quot;Best mean accuracy = &quot;+str(round(score,4))+&quot; (with 5-fold cross validation).&quot;) print(&quot; found after &quot; + str(round(partial - start,2)) + &quot; seconds&quot;) print() end = time.time() clf_svm = RandomForestClassifier(random_state=10, min_samples_leaf=best_values[&#39;min_sample_leaf&#39;], min_samples_split=best_values[&#39;min_sample_split&#39;], max_depth=best_values[&#39;max_depth&#39;], n_estimators=best_values[&#39;n_estimators&#39;]).fit(train, train_y) print(&quot;The best values are:&quot;) print(best_values) print(&quot;Best mean accuracy = &quot; + str(round(best_score,4)) +&quot; (with 5-fold cross validation).&quot;) print(&quot; found in &quot; + str(round(end - start,2)) + &quot; seconds.&quot;) . New best set of values {&#39;min_sample_leaf&#39;: 1, &#39;min_sample_split&#39;: 39, &#39;max_depth&#39;: 79, &#39;n_estimators&#39;: 108} Best mean accuracy = 0.942 with 5-fold cross validation. found after 1.61 seconds New best set of values {&#39;min_sample_leaf&#39;: 21, &#39;min_sample_split&#39;: 45, &#39;max_depth&#39;: 47, &#39;n_estimators&#39;: 98} Best mean accuracy = 0.945 with 5-fold cross validation. found after 4.04 seconds New best set of values {&#39;min_sample_leaf&#39;: 3, &#39;min_sample_split&#39;: 17, &#39;max_depth&#39;: 40, &#39;n_estimators&#39;: 170} Best mean accuracy = 0.95 with 5-fold cross validation. found after 43.83 seconds New best set of values {&#39;min_sample_leaf&#39;: 4, &#39;min_sample_split&#39;: 2, &#39;max_depth&#39;: 10, &#39;n_estimators&#39;: 148} Best mean accuracy = 0.952 with 5-fold cross validation. found after 49.18 seconds New best set of values {&#39;min_sample_leaf&#39;: 1, &#39;min_sample_split&#39;: 9, &#39;max_depth&#39;: 39, &#39;n_estimators&#39;: 124} Best mean accuracy = 0.957 with 5-fold cross validation. found after 98.83 seconds The best values are: {&#39;min_sample_leaf&#39;: 1, &#39;min_sample_split&#39;: 9, &#39;max_depth&#39;: 39, &#39;n_estimators&#39;: 124} Best mean accuracy = 0.957 with 5-fold cross validation. found in 158.75 seconds. . 4.3. KNearest Neighbors .",
            "url": "https://fcboss.github.io/data-science-blog/2020/10/08/Biostatistics-Project.html",
            "relUrl": "/2020/10/08/Biostatistics-Project.html",
            "date": " • Oct 8, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://fcboss.github.io/data-science-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://fcboss.github.io/data-science-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}